#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
é—²é±¼çˆ¬è™« - ç¬¬5éƒ¨åˆ† (å¹³æ¿ç”µè„‘)
ç›®æ ‡ï¼š100ä¸ªå•†å“ï¼Œ40ä¸ªç”¨æˆ·
"""

from goofish_base import GoofishBaseScraper

class GoofishPart5(GoofishBaseScraper):
    def __init__(self):
        super().__init__("part5")
        self.target_products = 100
        self.target_users = 40

    def run_scraping(self):
        """è¿è¡Œç¬¬5éƒ¨åˆ†çˆ¬å–"""
        print(f"\n{'='*60}")
        print(f"ðŸ¨ é—²é±¼çˆ¬è™« - ç¬¬5éƒ¨åˆ† (å¹³æ¿ç”µè„‘)")
        print(f"{'='*60}")
        print(f"ç›®æ ‡: {self.target_products} å•†å“, {self.target_users} ç”¨æˆ·")
        print(f"{'='*60}")

        # ç¬¬5éƒ¨åˆ†å…³é”®è¯ï¼šå¹³æ¿ç”µè„‘
        keywords = [
            "å¹³æ¿ç”µè„‘", "iPad", "iPad Pro", "iPad Air", "iPad mini",
            "åŽä¸ºå¹³æ¿", "å°ç±³å¹³æ¿", "å®‰å“å¹³æ¿", "äºŒæ‰‹å¹³æ¿", "å¹³æ¿äºŒæ‰‹"
        ]

        total_scraped = 0

        for keyword in keywords:
            if total_scraped >= self.target_products:
                break

            remaining = self.target_products - total_scraped
            batch_size = min(15, remaining)

            scraped = self.scrape_search_results(keyword, batch_size)
            total_scraped += scraped

            # ä¿å­˜è¿›åº¦
            self.save_progress(self.target_products)

            # å…³é”®è¯é—´ä¼‘æ¯
            if scraped > 0 and total_scraped < self.target_products:
                print(f"[ä¼‘æ¯] å…³é”®è¯é—´ä¼‘æ¯...")
                self.smart_delay(self.page_delay)

        print(f"\n{'='*60}")
        print(f"ðŸŽ‰ ç¬¬5éƒ¨åˆ†çˆ¬å–å®Œæˆ!")
        print(f"æ€»è®¡å•†å“: {len(self.products)}")
        print(f"æ€»è®¡ç”¨æˆ·: {len(set(u['email'] for u in self.users))}")
        print(f"å®ŒæˆçŽ‡: {len(self.products)/self.target_products*100:.1f}%")
        print(f"{'='*60}")

def main():
    scraper = GoofishPart5()

    try:
        if not scraper.connect_to_chrome():
            print("[å¤±è´¥] æ— æ³•è¿žæŽ¥åˆ°Chromeæµè§ˆå™¨")
            print("[æç¤º] è¯·ç¡®ä¿Chromeå·²å¯åŠ¨è¿œç¨‹è°ƒè¯•æ¨¡å¼")
            return

        scraper.run_scraping()

    except KeyboardInterrupt:
        print("\n[ä¸­æ–­] ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜å½“å‰è¿›åº¦...")
        scraper.save_progress(scraper.target_products)
    except Exception as e:
        print(f"[é”™è¯¯] ç¨‹åºå¼‚å¸¸: {e}")
        scraper.save_progress(scraper.target_products)

if __name__ == "__main__":
    main()