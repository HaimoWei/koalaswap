#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
é—²é±¼å®Œæ•´çˆ¬è™« - åˆå¹¶ç‰ˆæœ¬
ç›®æ ‡ï¼š500ä¸ªå•†å“ï¼Œ200ä¸ªç”¨æˆ·
é€‚åˆæŒ‚æœºè¿è¡Œï¼ŒåŒ…å«æ‰€æœ‰å…³é”®è¯
"""

from goofish_base import GoofishBaseScraper
import time
import random
from datetime import datetime

class GoofishComplete(GoofishBaseScraper):
    def __init__(self):
        super().__init__("complete")
        self.target_products = 800
        self.target_users = 300


    def run_complete_scraping(self):
        """è¿è¡Œå®Œæ•´çˆ¬å–"""
        print(f"\n{'='*60}")
        print(f"ğŸ¨ é—²é±¼å®Œæ•´çˆ¬è™« - æŒ‚æœºç‰ˆæœ¬")
        print(f"{'='*60}")
        print(f"ç›®æ ‡: {self.target_products} å•†å“, {self.target_users} ç”¨æˆ·")
        print(f"é¢„è®¡è¿è¡Œæ—¶é—´: 3-4å°æ—¶")
        print(f"åŒ…å«: æ•°ç /æœè£…/é‹ç±»/ç¾å¦†/æ—¥ç”¨å“/å®¶å±…/è¿åŠ¨ç­‰40ä¸ªç±»åˆ«")
        print(f"{'='*60}")

        # å®Œæ•´å…³é”®è¯åˆ—è¡¨ - 40ä¸ªå¤šç±»åˆ«å…³é”®è¯ï¼Œæ¯ä¸ªæŠ“20ä¸ªå•†å“
        all_keywords = [
            # æ•°ç ç±» (12ä¸ª)
            "iPhoneæ‰‹æœº", "åä¸ºæ‰‹æœº", "å°ç±³æ‰‹æœº", "OPPOæ‰‹æœº", "vivoæ‰‹æœº", "ä¸‰æ˜Ÿæ‰‹æœº",
            "iPad", "åä¸ºå¹³æ¿", "å°ç±³å¹³æ¿", "è€³æœº", "å……ç”µå®", "è“ç‰™éŸ³ç®±",

            # æœè£…ç±» (8ä¸ª)
            "è¿è¡£è£™", "Tæ¤", "ç‰›ä»”è£¤", "å¤–å¥—", "å«è¡£", "è¡¬è¡«", "æ¯›è¡£", "è£™å­",

            # é‹ç±» (6ä¸ª)
            "è€å…‹é‹", "é˜¿è¿ªè¾¾æ–¯", "è¿åŠ¨é‹", "å¸†å¸ƒé‹", "é«˜è·Ÿé‹", "é´å­",

            # ç¾å¦†æŠ¤è‚¤ (6ä¸ª)
            "åŒ–å¦†å“", "å£çº¢", "æŠ¤è‚¤å“", "é¦™æ°´", "é¢è†œ", "æ´—é¢å¥¶",

            # ç”Ÿæ´»ç”¨å“ (8ä¸ª)
            "ä¹¦åŒ…", "èƒŒåŒ…", "é’±åŒ…", "æ‰‹è¡¨", "çœ¼é•œ", "å¸½å­", "å›´å·¾", "æ‰‹å¥—"
        ]

        total_scraped = 0
        session_start_time = time.time()

        print(f"[å¼€å§‹] å…± {len(all_keywords)} ä¸ªå…³é”®è¯å¾…å¤„ç†")

        # è®¾ç½®æ¯ä¸ªå…³é”®è¯çš„ç›®æ ‡åˆ†é…
        target_per_keyword = self.target_products // len(all_keywords)  # å¹³å‡åˆ†é…
        keyword_targets = {}

        for keyword in all_keywords:
            keyword_targets[keyword] = target_per_keyword

        # ç»™å‰å‡ ä¸ªå…³é”®è¯å¤šåˆ†é…ä¸€äº›ï¼Œç¡®ä¿æ€»æ•°è¾¾æ ‡
        extra_needed = self.target_products - (target_per_keyword * len(all_keywords))
        for i in range(extra_needed):
            keyword_targets[all_keywords[i]] += 1

        print(f"[ç­–ç•¥] æ¯ä¸ªå…³é”®è¯ç›®æ ‡å•†å“æ•°: {target_per_keyword} ä¸ª (40ä¸ªå…³é”®è¯ Ã— 20ä¸ªå•†å“)")

        for i, keyword in enumerate(all_keywords, 1):
            if total_scraped >= self.target_products:
                break

            keyword_target = keyword_targets[keyword]
            keyword_scraped = 0
            attempts = 0
            max_attempts = 3

            # æ˜¾ç¤ºè¿›åº¦
            remaining = self.target_products - total_scraped
            progress = total_scraped / self.target_products * 100
            elapsed_time = (time.time() - session_start_time) / 3600

            print(f"\nğŸ“Š è¿›åº¦æŠ¥å‘Š [{i}/{len(all_keywords)}]")
            print(f"   å…³é”®è¯: {keyword} (ç›®æ ‡: {keyword_target} ä¸ª)")
            print(f"   å·²å®Œæˆ: {total_scraped}/{self.target_products} ({progress:.1f}%)")
            print(f"   å‰©ä½™: {remaining} ä¸ªå•†å“")
            print(f"   è¿è¡Œæ—¶é—´: {elapsed_time:.1f} å°æ—¶")

            # ç®€åŒ–ï¼šç›´æ¥å°è¯•è·å–ç›®æ ‡æ•°é‡çš„å•†å“
            remaining_for_keyword = min(keyword_target, self.target_products - total_scraped)

            if remaining_for_keyword > 0:
                print(f"[å°è¯•] å…³é”®è¯ '{keyword}' ç›®æ ‡è·å– {remaining_for_keyword} ä¸ªå•†å“")
                scraped = self.scrape_search_results(keyword, remaining_for_keyword)
                keyword_scraped = scraped
                total_scraped += scraped

                # ä¿å­˜è¿›åº¦
                self.save_progress(self.target_products)

                if scraped > 0:
                    print(f"[æˆåŠŸ] å…³é”®è¯ '{keyword}' è·å¾— {scraped} ä¸ªå•†å“")
                else:
                    print(f"[è­¦å‘Š] å…³é”®è¯ '{keyword}' æœªæ‰¾åˆ°å•†å“")

            print(f"[å®Œæˆ] å…³é”®è¯ '{keyword}' æ€»å…±è·å¾— {keyword_scraped}/{keyword_target} ä¸ªå•†å“")

            # å…³é”®è¯é—´ä¼‘æ¯
            if total_scraped < self.target_products:
                print(f"[ä¼‘æ¯] å…³é”®è¯é—´ä¼‘æ¯...")
                self.smart_delay(self.page_delay)

            # æ¯çˆ¬å–50ä¸ªå•†å“åé•¿ä¼‘æ¯ä¸€æ¬¡
            if total_scraped > 0 and total_scraped % 50 == 0:
                print(f"[é•¿ä¼‘æ¯] å·²çˆ¬å– {total_scraped} ä¸ªå•†å“ï¼Œé•¿ä¼‘æ¯ 2 åˆ†é’Ÿ...")
                time.sleep(120)

        # æœ€ç»ˆç»Ÿè®¡
        elapsed_hours = (time.time() - session_start_time) / 3600
        unique_users = len(set(u['email'] for u in self.users))

        print(f"\n{'='*60}")
        print(f"ğŸ‰ å®Œæ•´çˆ¬å–å®Œæˆ!")
        print(f"{'='*60}")
        print(f"ğŸ“ˆ æœ€ç»ˆç»Ÿè®¡:")
        print(f"   æ€»å•†å“æ•°: {len(self.products)}")
        print(f"   æ€»ç”¨æˆ·æ•°: {unique_users}")
        print(f"   ç›®æ ‡å®Œæˆç‡: {len(self.products)/self.target_products*100:.1f}%")
        print(f"   æ€»è¿è¡Œæ—¶é—´: {elapsed_hours:.1f} å°æ—¶")
        print(f"   å¹³å‡æ¯å•†å“: {elapsed_hours*3600/len(self.products):.1f} ç§’")
        print(f"{'='*60}")
        print(f"ğŸ“ æ•°æ®æ–‡ä»¶:")
        print(f"   - dataset/products_complete.json")
        print(f"   - dataset/users_complete.json")
        print(f"   - dataset/images/ ({len(self.products)} å¼ å›¾ç‰‡)")
        print(f"{'='*60}")

        # æ˜¾ç¤ºéƒ¨åˆ†å•†å“ç¤ºä¾‹
        if self.products:
            print(f"\nğŸ“± å•†å“ç¤ºä¾‹:")
            for i, product in enumerate(self.products[:5]):
                print(f"  {i+1}. {product['title'][:50]}... - ${product['price']} AUD")

def main():
    import time

    scraper = GoofishComplete()

    try:
        if not scraper.connect_to_chrome():
            print("[å¤±è´¥] æ— æ³•è¿æ¥åˆ°Chromeæµè§ˆå™¨")
            print("[æç¤º] è¯·ç¡®ä¿Chromeå·²å¯åŠ¨è¿œç¨‹è°ƒè¯•æ¨¡å¼")
            print("       å‘½ä»¤: \"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\" --remote-debugging-port=9222 --user-data-dir=\"C:\\temp\\chrome_debug\"")
            return

        print("âš ï¸  æŒ‚æœºçˆ¬å–æé†’:")
        print("   - ç¡®ä¿ç”µè„‘ä¸ä¼šä¼‘çœ ")
        print("   - ç¡®ä¿ç½‘ç»œè¿æ¥ç¨³å®š")
        print("   - é¢„è®¡è¿è¡Œ 3-4 å°æ—¶ (40ä¸ªå…³é”®è¯ç­–ç•¥)")
        print("   - å¯ä»¥éšæ—¶æŒ‰ Ctrl+C å®‰å…¨ä¸­æ–­")

        input("\næŒ‰å›è½¦é”®å¼€å§‹æŒ‚æœºçˆ¬å–...")

        scraper.run_complete_scraping()

        print(f"\nâœ… æŒ‚æœºçˆ¬å–æˆåŠŸå®Œæˆ!")
        print(f"ğŸ’¤ å¯ä»¥å®‰å¿ƒç¡è§‰äº†ï¼Œæ•°æ®å·²å…¨éƒ¨ä¿å­˜!")

    except KeyboardInterrupt:
        print(f"\n[ä¸­æ–­] ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜å½“å‰è¿›åº¦...")
        scraper.save_progress(scraper.target_products)
        print(f"ğŸ’¾ è¿›åº¦å·²ä¿å­˜ï¼Œå¯ä»¥ç¨åç»§ç»­è¿è¡Œç›¸åŒè„šæœ¬")
    except Exception as e:
        print(f"[é”™è¯¯] ç¨‹åºå¼‚å¸¸: {e}")
        scraper.save_progress(scraper.target_products)
        print(f"ğŸ’¾ è¿›åº¦å·²ä¿å­˜")

if __name__ == "__main__":
    main()