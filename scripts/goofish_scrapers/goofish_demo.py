#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
é—²é±¼çˆ¬è™« - Demoç‰ˆæœ¬
ä»…çˆ¬å–10ä¸ªå•†å“ç”¨äºæµ‹è¯•
"""

from goofish_base import GoofishBaseScraper

class GoofishDemo(GoofishBaseScraper):
    def __init__(self):
        super().__init__("demo")
        self.target_products = 10
        self.target_users = 10

    def run_demo_scraping(self):
        """è¿è¡ŒDemoçˆ¬å–"""
        print(f"\n{'='*60}")
        print(f"ğŸ§ª é—²é±¼çˆ¬è™« - Demoæµ‹è¯•ç‰ˆæœ¬")
        print(f"{'='*60}")
        print(f"ç›®æ ‡: {self.target_products} å•†å“, {self.target_users} ç”¨æˆ·")
        print(f"ç”¨é€”: æµ‹è¯•çˆ¬è™«åŠŸèƒ½å’Œæ•°æ®è´¨é‡")
        print(f"{'='*60}")

        # Demoå…³é”®è¯ï¼šä½¿ç”¨æ›´ä¸åŒçš„å…³é”®è¯
        keywords = ["iPhoneæ‰‹æœº", "åä¸ºæ‰‹æœº", "å°ç±³æ‰‹æœº"]

        total_scraped = 0

        for keyword in keywords:
            if total_scraped >= self.target_products:
                break

            remaining = self.target_products - total_scraped
            batch_size = min(5, remaining)  # æ¯ä¸ªå…³é”®è¯æœ€å¤š5ä¸ªï¼Œç¡®ä¿å¤šæ ·æ€§

            print(f"\n[Demo] æµ‹è¯•å…³é”®è¯: {keyword}")
            scraped = self.scrape_search_results(keyword, batch_size)
            total_scraped += scraped

            # ä¿å­˜è¿›åº¦
            self.save_progress(self.target_products)

            if total_scraped >= self.target_products:
                break

            # å…³é”®è¯é—´çŸ­æš‚ä¼‘æ¯
            if scraped > 0:
                print(f"[Demo] çŸ­æš‚ä¼‘æ¯...")
                self.smart_delay(5)  # Demoç‰ˆæœ¬ä¼‘æ¯æ—¶é—´æ›´çŸ­

        print(f"\n{'='*60}")
        print(f"ğŸ‰ Demoæµ‹è¯•å®Œæˆ!")
        print(f"æ€»è®¡å•†å“: {len(self.products)}")
        print(f"æ€»è®¡ç”¨æˆ·: {len(set(u['email'] for u in self.users))}")
        print(f"å®Œæˆç‡: {len(self.products)/self.target_products*100:.1f}%")
        print(f"{'='*60}")
        print(f"ğŸ“ æ•°æ®æ–‡ä»¶:")
        print(f"  - dataset/products_demo.json")
        print(f"  - dataset/users_demo.json")
        print(f"  - dataset/images/ (å•†å“å›¾ç‰‡)")
        print(f"{'='*60}")

        # æ˜¾ç¤ºç¤ºä¾‹å•†å“
        if self.products:
            print(f"\nğŸ“± ç¤ºä¾‹å•†å“:")
            for i, product in enumerate(self.products[:3]):
                print(f"  {i+1}. {product['title'][:50]}...")
                print(f"     ä»·æ ¼: ${product['price']} AUD")
                print(f"     å›¾ç‰‡: {len(product['images'])} å¼ ")

def main():
    scraper = GoofishDemo()

    try:
        if not scraper.connect_to_chrome():
            print("[å¤±è´¥] æ— æ³•è¿æ¥åˆ°Chromeæµè§ˆå™¨")
            print("[æç¤º] è¯·ç¡®ä¿Chromeå·²å¯åŠ¨è¿œç¨‹è°ƒè¯•æ¨¡å¼")
            print("       å‘½ä»¤: \"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\" --remote-debugging-port=9222 --user-data-dir=\"C:\\temp\\chrome_debug\"")
            return

        scraper.run_demo_scraping()

        print(f"\nâœ… Demoæµ‹è¯•æˆåŠŸå®Œæˆ!")
        print(f"ğŸ’¡ å¦‚æœæ•°æ®è´¨é‡æ»¡æ„ï¼Œå¯ä»¥è¿è¡Œç”Ÿäº§ç‰ˆæœ¬:")
        print(f"   python scripts/goofish_part1.py")
        print(f"   python scripts/goofish_part2.py")
        print(f"   ... (ä¾æ¬¡è¿è¡Œpart1-part5)")

    except KeyboardInterrupt:
        print("\n[ä¸­æ–­] ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜å½“å‰è¿›åº¦...")
        scraper.save_progress(scraper.target_products)
    except Exception as e:
        print(f"[é”™è¯¯] ç¨‹åºå¼‚å¸¸: {e}")
        scraper.save_progress(scraper.target_products)

if __name__ == "__main__":
    main()